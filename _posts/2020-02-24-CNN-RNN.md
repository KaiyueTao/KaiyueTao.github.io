---
layout: post
title: CNN-RNN
author: Kaiyue Tao
tags: [Computer Vision, Learning, Multi-label Classification]
math: true
mermaid: true
---

**Arthur: Jiang Wang, Yi Yang, etc

available at [Here](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Wang_CNN-RNN_A_Unified_CVPR_2016_paper.html)

## 主要贡献

1. 提出了end-to-end的多标签图像分类框架
2. 运用了recurrent neurons，更好的适用于high-order label co-occurrence dependency
3. rnn隐含了attention机制，对于小目标更有效（之后作者通过de-cnn反向视觉化了RNN之后的图，发现网络对于图片的关注点确实跟类别有关，interesting results）

## Motivation

1. common approach: transform into multiple single-label classification problems. 但是这种模型没有考虑标签之间的关联性（label dependency），因为**多标签分类的某些标签之间总是强烈相关的**，如sky和cloud。

2. 针对于label dependency的**图模型**，在标签集很大的时候，parameter过大，但事实上很多是多余的。且在计算复杂度和higher-order correlations上不能兼顾。

3. 在CNN模型中，一般假设所有分类器都共享同一个feature map，但这会导致小目标的丢失。

## Idea

1. adapt attentional area in images

    当predict主要的part之后，shift to smaller one，这样虽然小目标本身很难检测，但是给了足够的文本后就容易多了（主目标等），这个在RNN中隐含地实现了。

## Model

<img src="https://tva1.sinaimg.cn/large/0082zybpgy1gc7ifys3lgj311k0qwgtp.jpg" />

通过CNN生成image的vectors，每个label都有其对应的vector，并进行image embedding。再由RNN计算多标签的probability，用beam search algorithm找到probability最高的。

## Method

1. Long Short Term Memory Networks (LSTM)

    RNN是用于处理序列数据的神经网络，而LSTM则在RNN的神经元上加入了3个gate：
    - **f**: forget gate 是否忘记当前状态
    - **i**: 是否读入input
    - **o**: 是否输出

    $$
    \begin{aligned} x_{t} &=\delta\left(U_{r} \cdot r(t-1)+U_{w} w_{k}(t)\right) \\ i_{t} &=\delta\left(U_{i_{r}} r(t-1)+U_{i_{w}} w_{k}(t)\right) \\ f_{t} &=\delta\left(U_{f_{r}} r(t-1)+U_{f_{w}} w_{k}(t)\right) \\ o_{t} &=\delta\left(U_{o_{r}} r(t-1)+U_{o_{w}} w_{k}(t)\right) \\ r(t) &=f_{t} \odot r(t-1)+i_{t} \odot x_{t} \\ o(t) &=r(t) \odot o(t) \end{aligned}
    $$

    ReLU as activation func.

2. CNN-RNN framework

    <img src="https://tva1.sinaimg.cn/large/0082zybpgy1gc7jpl0596j30u00gwmzt.jpg" />

    上图表示的其实是一个label的预测过程，相当于t时刻的预测，对于一张图片，经过若干次预测后，最终到END标签结束。

    CNN用于提取图像的feature。
    
    RNN则用来结合当前Label（也就是上一个预测的label）和之前label的hidden information，得到一个在embedding基础上的output，即label间的一个共存关系（coouccurency）。label embbeding可以通过一个矩阵$U_l$来表示：

    $$
    w_{k}=U_{l} \cdot e_{k}
    $$

    （$e_k$表示第k个label的one-hot vector，$U_l$的第k行就是label k的embedding，则$w_k$就是当前的embedding）

    经过recurrent layer后得到对应的output，因为RNN的记忆机制，这里的output其实隐含了之前预测的label的信息：

    $$
    o(t)=h_{o}\left(r(t-1), w_{k}(t)\right), r(t)=h_{r}\left(r(t-1), w_{k}(t)\right)
    $$

    最后RNN的output和CNN的feature结合，实际上就是相加，输入Projection Layer。

    $$
    x_{t}=h\left(U_{o}^{x} o(t)+U_{I}^{x} I\right)
    $$

    （其中I表示CNN提取的feature）

    然后乘上$U_l$的转置得到最终的label probability：

    $$
    s(t)=U_{l}^{T} x_{t}
    $$

3. beam search algorithm

    显然，因为prediction path是一个寻找label序列的过程，所以要找到probability最大的label序列，相当于：

    $$
    \begin{aligned} l_{1}, \cdots, l_{k} &=\arg \max _{l_{1}, \ldots, l_{k}} P\left(l_{1}, \cdots, l_{k} | I\right) \\ &=\arg \max _{l_{1}, \cdots, l_{k}} P\left(l_{1} | I\right) \times P\left(l_{2} | I, l_{1}\right) \\ & \cdots P\left(l_{k} | I, l_{1}, \cdots, l_{k-1}\right) \end{aligned}
    $$

    显然这个P值并非Markov性质（并不是只与前一个有关的），所以只能采用一个greedy approximation策略（贪婪近似），也就是说每一步都找最大的（在t步找$l_t = argmax_{l_t}P(l_t | I,l_1,...,l_t)$，t+1步则从t基础上继续找）。这样的问题在于第一个错了可能会全错，故有beam search algorithm：

    **在每一步找前N个P值最高的序列，在t+1步则得到$N*N$个path，然后再选出N个，在这个过程中，遇到END标志的path加入candidate path的集合中。当当前的所有path的P值都小于candidate中的所有Path时，算法停止。**

    （*其实是个有些粗暴的greedy算法的修正*）

## Training

1. loss
   
    对于该模型，一个重点在于loss的计算，因为文章没有讲得很清楚，这也是我一开始没有理解的。

    文章采用的是crossentropyloss，但是因为这是多标签，实际上它是对：

    $$
    \mathcal{S}(t)=\left\{P_{1}(t), P_{2}(t), \cdots, P_{N}(t)\right\}
    $$

    做了一个softmax，即$softmax(s(t))$，而s(t)如上面的公式，是t时刻对N个path的预测，也就是说，target是t时刻的label序列，假如target有三个label，t=2，就应该取前2个作为target。

    那么其实这里很跟label的顺序有很大关系的，因为同样的cat,dog和dog,cat显然在这里并不等同。我的理解是这里“强制”让model去学习一个按标签顺序来的预测方式，也就是取决于原图的标签顺序。

2. order of label

    因为training涉及到label order的问题，在文章中采用的是根据出现频率进行标签：

    More frequent labels appear earlier than less frequent one.

    他们认为，这样也体现了预测更容易的label对后续有一个帮助，但是这里frequency和easier是不是等同的我认为有待商榷。

    当然他们也做了实验，将label从一个标记上难易程度去排序（感觉这里才是easy to hard），以及做一个ramdom排序，但是发现并没有提升。而对每个mini-batch去做一个打乱，则让训练非常困难。

## Expriment

说一下文章前面提到的RNN的hidden attention特性，在第一个label之后，注意力发生了改变，蛮有意思的：

<img src="https://tva1.sinaimg.cn/large/0082zybpgy1gc8126nz96j311y0ec7f2.jpg" />

- MS COCO dataset - 

<img src="https://tva1.sinaimg.cn/large/00831rSTgy1gcc8mm4rw1j30r40aedhp.jpg" />

- PASCAL VOC 2007

<img src="https://tva1.sinaimg.cn/large/00831rSTgy1gcc8rldub5j31lk0b2n22.jpg" />


## 思考

文章用了RNN的特点去完成一个label cooccurency的分析，这点上很新颖。不过我认为文章最有趣的还是通过Deconvolution后得到的这个Attention map，发现注意力的转移，这可能也是这个RNN的label co-occurency的本质吧，所以我感觉multi-label预测的关键主要还是一个注意力机制，如果能够在这点上进行思考可能会对多标签有一个帮助和提升。



















